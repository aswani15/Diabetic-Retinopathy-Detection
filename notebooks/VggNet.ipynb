{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VggNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8-lnoZodepJ",
        "colab_type": "code",
        "outputId": "c73662fe-4a12-4fa6-b37b-a95392fb15d9",
        "colab": { 
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_7qf4MLZKLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "from IPython.display import clear_output\n",
        "from progressbar import progressbar as pro"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DljBEoyn1I60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LoadData(torch.utils.data.Dataset):\n",
        "    \n",
        "    def __init__(self, csv_file, root_dir):\n",
        "        self.aptos_frame = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.aptos_frame)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                self.aptos_frame.iloc[idx, 0])\n",
        "        image = cv2.imread(img_name + \".png\")\n",
        "        image = cv2.resize(image, (254,254))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "        image = clahe.apply(image)\n",
        "        #image = apply_entropy(image)\n",
        "        image = image[np.newaxis,:,:]\n",
        "        diagnosis = self.aptos_frame.iloc[idx, 1]\n",
        "        diagnosis = np.array([diagnosis])\n",
        "\n",
        "        sample = {'image': image, 'diagnosis': diagnosis}\n",
        "        \n",
        "        return sample\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUlkpLj91I1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_file = \"/content/gdrive/My Drive/modtrain.csv\"\n",
        "root_dir = \"/content/gdrive/My Drive/train\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxU_zAN11Iv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BarYhNX81Iq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = LoadData(csv_file, root_dir)\n",
        "dataloaders = torch.utils.data.DataLoader(train_data, batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLPFeH-RaUP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import matplotlib.cm as mpl_color_map\n",
        "import copy\n",
        "# from misc_functions import get_example_params, save_class_activation_images\n",
        "\n",
        "\n",
        "class CamExtractor():\n",
        "    \"\"\"\n",
        "        Extracts cam features from the model\n",
        "    \"\"\"\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "\n",
        "    def save_gradient(self, grad):\n",
        "        self.gradients = grad\n",
        "\n",
        "    def forward_pass_on_convolutions(self, x):\n",
        "        \"\"\"\n",
        "            Does a forward pass on convolutions, hooks the function at given layer\n",
        "        \"\"\"\n",
        "        conv_output = None\n",
        "        for module_pos, module in self.model.features._modules.items():\n",
        "            x = module(x)  # Forward\n",
        "            if int(module_pos) == self.target_layer:\n",
        "                x.register_hook(self.save_gradient)\n",
        "                conv_output = x  # Save the convolution output on that layer\n",
        "        return conv_output, x\n",
        "\n",
        "    def forward_pass(self, x):\n",
        "        \"\"\"\n",
        "            Does a full forward pass on the model\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # Forward pass on the convolutions\n",
        "        conv_output, x = self.forward_pass_on_convolutions(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        # Forward pass on the classifier\n",
        "\n",
        "        x = self.model.classifier(x)\n",
        "        return conv_output, x\n",
        "\n",
        "\n",
        "class GradCam():\n",
        "    \"\"\"\n",
        "        Produces class activation map\n",
        "    \"\"\"\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        # Define extractor\n",
        "        self.extractor = CamExtractor(self.model, target_layer)\n",
        "\n",
        "    def generate_cam(self, input_image, target_class=None):\n",
        "        # Full forward pass\n",
        "        # conv_output is the output of convolutions at specified layer\n",
        "        # model_output is the final output of the model (1, 1000)\n",
        "        conv_output, model_output = self.extractor.forward_pass(input_image)\n",
        "        if target_class is None:\n",
        "            target_class = np.argmax(model_output.data.numpy())\n",
        "        # Target for backprop\n",
        "        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_().to(device)\n",
        "        one_hot_output[0][target_class] = 1\n",
        "        # Zero grads\n",
        "        self.model.features.zero_grad()\n",
        "        self.model.classifier.zero_grad()\n",
        "        # Backward pass with specified target\n",
        "        model_output.backward(gradient=one_hot_output, retain_graph=True)\n",
        "        # Get hooked gradients\n",
        "        guided_gradients = self.extractor.gradients.data.cpu().numpy()[0]\n",
        "        # Get convolution outputs\n",
        "        target = conv_output.data.cpu().numpy()[0]\n",
        "        # Get weights from gradients\n",
        "        weights = np.mean(guided_gradients, axis=(1, 2))  # Take averages for each gradient\n",
        "        # Create empty numpy array for cam\n",
        "        cam = np.ones(target.shape[1:], dtype=np.float32)\n",
        "        # Multiply each weight with its conv output and then, sum\n",
        "        for i, w in enumerate(weights):\n",
        "            cam += w * target[i, :, :]\n",
        "        cam = np.maximum(cam, 0)\n",
        "        cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam))  # Normalize between 0-1\n",
        "        cam = np.uint8(cam * 255)  # Scale between 0-255 to visualize\n",
        "        cam = np.uint8(Image.fromarray(cam).resize((input_image.shape[2],\n",
        "                       input_image.shape[3]), Image.ANTIALIAS))/255\n",
        "        # ^ I am extremely unhappy with this line. Originally resizing was done in cv2 which\n",
        "        # supports resizing numpy matrices with antialiasing, however,\n",
        "        # when I moved the repository to PIL, this option was out of the window.\n",
        "        # So, in order to use resizing with ANTIALIAS feature of PIL,\n",
        "        # I briefly convert matrix to PIL image and then back.\n",
        "        # If there is a more beautiful way, do not hesitate to send a PR.\n",
        "        return cam\n",
        "\n",
        "def preprocess_image(img):\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize(\n",
        "                                                              mean=[0.456],\n",
        "                                                              std= [0.225])])\n",
        "    img = trans(img)\n",
        "    img.unsqueeze_(0)\n",
        "    return img\n",
        "\n",
        "def save_class_activation_images(org_img, activation_map, file_name, epo):\n",
        "    \"\"\"\n",
        "        Saves cam activation map and activation map on the original image\n",
        "    Args:\n",
        "        org_img (PIL img): Original image\n",
        "        activation_map (numpy arr): Activation map (grayscale) 0-255\n",
        "        file_name (str): File name of the exported image\n",
        "    \"\"\"\n",
        "    if not os.path.exists('/content/gdrive/My Drive/models/results'):\n",
        "        os.makedirs('/content/gdrive/My Drive/models/results')\n",
        "    # Grayscale activation map\n",
        "    heatmap, heatmap_on_image = apply_colormap_on_image(org_img, activation_map, 'hsv')\n",
        "    \n",
        "    # Save heatmap on iamge\n",
        "    path_to_file = os.path.join('/content/gdrive/My Drive/models/results', file_name+f'_{epo}_epoch.png')\n",
        "    save_image(heatmap_on_image, path_to_file)\n",
        "\n",
        "def apply_colormap_on_image(org_im, activation, colormap_name):\n",
        "    \"\"\"\n",
        "        Apply heatmap on image\n",
        "    Args:\n",
        "        org_img (PIL img): Original image\n",
        "        activation_map (numpy arr): Activation map (grayscale) 0-255\n",
        "        colormap_name (str): Name of the colormap\n",
        "    \"\"\"\n",
        "    # Get colormap\n",
        "    org_im = Image.fromarray(org_im).resize((224,224))\n",
        "\n",
        "    color_map = mpl_color_map.get_cmap(colormap_name)\n",
        "    no_trans_heatmap = color_map(activation)\n",
        "    # Change alpha channel in colormap to make sure original image is displayed\n",
        "    heatmap = copy.copy(no_trans_heatmap)\n",
        "    heatmap[:, :, 3] = 0.4\n",
        "    heatmap = Image.fromarray((heatmap*255).astype(np.uint8))\n",
        "    no_trans_heatmap = Image.fromarray((no_trans_heatmap*255).astype(np.uint8))\n",
        "\n",
        "    # Apply heatmap on iamge\n",
        "    heatmap_on_image = Image.new(\"RGBA\", org_im.size)\n",
        "    \n",
        "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, org_im.convert('RGBA'))\n",
        "\n",
        "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, heatmap)\n",
        "    return no_trans_heatmap, heatmap_on_image\n",
        "\n",
        "def save_image(im, path):\n",
        "    \"\"\"\n",
        "        Saves a numpy matrix or PIL image as an image\n",
        "    Args:\n",
        "        im_as_arr (Numpy array): Matrix of shape DxWxH\n",
        "        path (str): Path to the image\n",
        "    \"\"\"\n",
        "    if isinstance(im, (np.ndarray, np.generic)):\n",
        "        im = format_np_output(im)\n",
        "        im = Image.fromarray(im)\n",
        "    im.save(path)\n",
        "\n",
        "def format_np_output(np_arr):\n",
        "    \"\"\"\n",
        "        This is a (kind of) bandaid fix to streamline saving procedure.\n",
        "        It converts all the outputs to the same format which is 3xWxH\n",
        "        with using sucecssive if clauses.\n",
        "    Args:\n",
        "        im_as_arr (Numpy array): Matrix of shape 1xWxH or WxH or 3xWxH\n",
        "    \"\"\"\n",
        "    # Phase/Case 1: The np arr only has 2 dimensions\n",
        "    # Result: Add a dimension at the beginning\n",
        "    if len(np_arr.shape) == 2:\n",
        "        np_arr = np.expand_dims(np_arr, axis=0)\n",
        "    # Phase/Case 2: Np arr has only 1 channel (assuming first dim is channel)\n",
        "    # Result: Repeat first channel and convert 1xWxH to 3xWxH\n",
        "    if np_arr.shape[0] == 1:\n",
        "        np_arr = np.repeat(np_arr, 3, axis=0)\n",
        "    # Phase/Case 3: Np arr is of shape 3xWxH\n",
        "    # Result: Convert it to WxHx3 in order to make it saveable by PIL\n",
        "    if np_arr.shape[0] == 3:\n",
        "        np_arr = np_arr.transpose(1, 2, 0)\n",
        "    # Phase/Case 4: NP arr is normalized between 0-1\n",
        "    # Result: Multiply with 255 and change type to make it saveable by PIL\n",
        "    if np.max(np_arr) <= 1:\n",
        "        np_arr = (np_arr*255).astype(np.uint8)\n",
        "    return np_arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5luXstc1Imh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, criterion, optimizer, num_epochs=25):\n",
        "    since = time.time()\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    overall_acc ={}\n",
        "    \n",
        "    for j in range(num_epochs):\n",
        "      train_loss = 0\n",
        "      correct = 0\n",
        "      acc_data_train = []\n",
        "      train_losses = []\n",
        "      for i, sample_batched in enumerate(dataloaders,1):\n",
        "          clear_output()\n",
        "          print(f\"{i} of {len(dataloaders)}\")\n",
        "\n",
        "          inputs = sample_batched['image'].to(device)\n",
        "          labels = sample_batched['diagnosis'].squeeze(1).to(device)\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = model(inputs.float())\n",
        "\n",
        "          loss = criterion(F.log_softmax(outputs, dim=1), labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          train_loss+= loss.item()\n",
        "          pred = outputs.data.max(1, keepdim=True)[1].int()\n",
        "          labels = labels.int()\n",
        "          #print(labels)\n",
        "          correct += pred.eq(labels.data.view_as(pred)).sum()       \n",
        "\n",
        "\n",
        "\n",
        "      accuracy = float(100*float(correct)/3662)\n",
        "\n",
        "      acc_data_train.append([accuracy])\n",
        "      train_loss=float(train_loss)/float(i)\n",
        "      train_losses.append([optimizer, lr,w,train_loss])\n",
        "      # print statistics\n",
        "      print('Train Epoch:{}  Accuracy: ({}/{}) {:.2f}%   Average Loss: {:.2f} \\n'.\n",
        "            format(j, correct, 3662, accuracy, train_loss))\n",
        "      overall_acc[f'epoch_{j}'] = {}\n",
        "      overall_acc[f'epoch_{j}']['acc'] = accuracy\n",
        "      overall_acc[f'epoch_{j}']['loss'] = train_loss\n",
        "      \n",
        "      for layer in [2,8,13,20]:\n",
        "      \n",
        "        if __name__ == '__main__':\n",
        "          # Get params\n",
        "          target_example = 0  # Snake\n",
        "          #(original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n",
        "              #torch.load(\"alexnet_model.pth\")\n",
        "          # Grad cam\n",
        "\n",
        "          original_image = cv2.imread(\"/content/gdrive/My Drive/train/ff0740cb484a.png\", 0)\n",
        "\n",
        "          # prep_img\n",
        "          prep_img = preprocess_image(original_image).to(device)\n",
        "          target_class = 0\n",
        "\n",
        "          grad_cam = GradCam(model, target_layer=layer)\n",
        "          # Generate cam mask\n",
        "          cam = grad_cam.generate_cam(prep_img, target_class)\n",
        "          # Save mask\n",
        "\n",
        "          save_class_activation_images(original_image, cam, f\"grad_entropy_cam_layer_{layer}\",j)\n",
        "\n",
        "    return model, overall_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZSlZLck1IcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_ft = models.vgg11(pretrained=True, progress=True)\n",
        "#model_ft"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8R1i-Y0Jf2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param in model_ft.parameters():\n",
        "    param.requires_grad = False\n",
        "#Adjust Model can read gray scale images\n",
        "model_ft.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1,1), bias=False)\n",
        "#Adjust Output\n",
        "model_ft.classifier[6] = nn.Linear(in_features=4096, out_features=2, bias=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kddyYQSM1ftv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_ftrs = model_ft.classifier[6].in_features\n",
        "lr= 0.0003\n",
        "w = 6.7752e-06\n",
        "optimizer_ft = optim.Adam(model_ft.parameters(), lr=lr,weight_decay=w, amsgrad=True)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzUrv2uo1k0a",
        "colab_type": "code",
        "outputId": "2916cfcc-d5e1-4792-b3bc-8e07fcbb5f86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "model_ft, overall_acc = train_model(model_ft, criterion, optimizer_ft,\n",
        "                       num_epochs=25)\n",
        "torch.save(model_ft, \"/content/gdrive/My Drive/models/vgg_11_filter_entropy.pth\")\n",
        "import pickle\n",
        "f = open(\"/content/gdrive/My Drive/models/results_entropy_dict.pkl\",\"wb\")\n",
        "pickle.dump(overall_acc,f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58 of 58\n",
            "Train Epoch:24  Accuracy: (3456/3662) 94.37%   Average Loss: 0.16 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhjgvy_ADzwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPyT-GEn6Alp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_file = \"/content/gdrive/My Drive/models/validation_01.csv\"\n",
        "root_dir = \"/content/gdrive/My Drive/train\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVQJ3GkM6Kev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p19flBc6jmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = LoadData(csv_file, root_dir)\n",
        "dataloaders = torch.utils.data.DataLoader(train_data, batch_size=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhcGncbz8dJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = []\n",
        "model.to(device)\n",
        "for data in dataloaders:\n",
        "  labels.append(model(data['image'].to(device).float()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaY_u6_YFifC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(csv_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDLJBhjCHKG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = []\n",
        "model.to(device)\n",
        "for id_ in data.id_code:\n",
        "  image = cv2.imread(root_dir+\"/\"+id_+\".png\")\n",
        "  image = cv2.resize(image, (254,254))\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "  image = clahe.apply(image)\n",
        "  #image = apply_entropy(image)\n",
        "  image = image[np.newaxis,:,:]\n",
        "  image = torch.Tensor(image).to(device)\n",
        "  labels.append(model(image.float()))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgW4h0MDHMHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
